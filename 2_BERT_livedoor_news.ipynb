{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_BERT_livedoor_news.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykato27/BERT-Japanese/blob/main/2_BERT_livedoor_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYlVm0kpjx57"
      },
      "source": [
        "## 日本語BERTでlivedoorニュースを教師あり学習で分類"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqIW6ar2Bm3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fad6e5f-594b-4924-e72e-de0367f207b3"
      },
      "source": [
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f716d35f8b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-mqaAhCApI"
      },
      "source": [
        "### GPUの使用可能を確認\n",
        "\n",
        "画面上部のメニュー ランタイム > ランタイムのタイプを変更 で、 ノートブックの設定 を開く\n",
        "\n",
        "ハードウェアアクセラレータに GPU を選択し、 保存 する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8TJgawCB_Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db88d80f-606e-4c05-a906-030457ce000f"
      },
      "source": [
        "# GPUの使用確認：True or False\n",
        "torch.cuda.is_available()\n",
        "\n",
        "# TrueならGPU使用可能"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc6dbVUfj1W-"
      },
      "source": [
        "## 準備1：Livedoorニュースをダウンロードしてtsvファイル化\n",
        "\n",
        "参考：https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/finetune-to-livedoor-corpus.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vh0a49Gp-rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b49e3f2-bbd6-4a05-b7a3-03cf0b9fa5ad"
      },
      "source": [
        "# Livedoorニュースのファイルをダウンロード\n",
        "! wget \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-03 05:32:47--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
            "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
            "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8855190 (8.4M) [application/x-gzip]\n",
            "Saving to: ‘ldcc-20140209.tar.gz.2’\n",
            "\n",
            "ldcc-20140209.tar.g 100%[===================>]   8.44M  5.12MB/s    in 1.6s    \n",
            "\n",
            "2021-07-03 05:32:50 (5.12 MB/s) - ‘ldcc-20140209.tar.gz.2’ saved [8855190/8855190]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keY2WGdwjzLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d2a503-e501-4e14-f4c6-4b399a1bbe06"
      },
      "source": [
        "# ファイルを解凍し、カテゴリー数と内容を確認\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# 解凍\n",
        "tar = tarfile.open(\"ldcc-20140209.tar.gz\", \"r:gz\")\n",
        "tar.extractall(\"./data/livedoor/\")\n",
        "tar.close()\n",
        "\n",
        "# フォルダのファイルとディレクトリを確認\n",
        "files_folders = [name for name in os.listdir(\"./data/livedoor/text/\")]\n",
        "print(files_folders)\n",
        "\n",
        "# カテゴリーのフォルダのみを抽出\n",
        "categories = [name for name in os.listdir(\n",
        "    \"./data/livedoor/text/\") if os.path.isdir(\"./data/livedoor/text/\"+name)]\n",
        "\n",
        "print(\"カテゴリー数:\", len(categories))\n",
        "print(categories)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CHANGES.txt', 'livedoor-homme', 'it-life-hack', 'topic-news', 'movie-enter', 'smax', 'sports-watch', 'dokujo-tsushin', 'peachy', 'README.txt', 'kaden-channel']\n",
            "カテゴリー数: 9\n",
            "['livedoor-homme', 'it-life-hack', 'topic-news', 'movie-enter', 'smax', 'sports-watch', 'dokujo-tsushin', 'peachy', 'kaden-channel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z201OQ7gvYOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c74224-d79b-4ab4-de94-9ff85437c5d5"
      },
      "source": [
        "# ファイルの中身を確認してみる\n",
        "file_name = \"./data/livedoor/text/movie-enter/movie-enter-6255260.txt\"\n",
        "\n",
        "with open(file_name) as text_file:\n",
        "    text = text_file.readlines()\n",
        "    print(\"0：\", text[0])  # URL情報\n",
        "    print(\"1：\", text[1])  # タイムスタンプ\n",
        "    print(\"2：\", text[2])  # タイトル\n",
        "    print(\"3：\", text[3])  # 本文\n",
        "\n",
        "    # 今回は4要素目には本文は伸びていないが、4要素目以降に本文がある場合もある\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0： http://news.livedoor.com/article/detail/6255260/\n",
            "\n",
            "1： 2012-02-07T09:00:00+0900\n",
            "\n",
            "2： 新しいヴァンパイアが誕生！　ジョニデ主演『ダーク・シャドウ』の公開日が決定\n",
            "\n",
            "3： 　こんなヴァンパイアは見たことがない！　ジョニー・デップとティム・バートン監督がタッグを組んだ映画『ダーク・シャドウズ（原題）』の邦題が『ダーク・シャドウ』に決定。日本公開日が5月19日に決まった。さらに、ジョニー・デップ演じるヴァンパイアの写真が公開された。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoKvaAK1vurV"
      },
      "source": [
        "# 本文を取得する前処理関数を定義\n",
        "\n",
        "\n",
        "def extract_main_txt(file_name):\n",
        "    with open(file_name) as text_file:\n",
        "        # 今回はタイトル行は外したいので、3要素目以降の本文のみ使用\n",
        "        text = text_file.readlines()[3:]\n",
        "\n",
        "        # 3要素目以降にも本文が入っている場合があるので、リストにして、後で結合させる\n",
        "        text = [sentence.strip() for sentence in text]  # 空白文字(スペースやタブ、改行)の削除\n",
        "        text = list(filter(lambda line: line != '', text))\n",
        "        text = ''.join(text)\n",
        "        text = text.translate(str.maketrans(\n",
        "            {'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))  # 改行やタブ、全角スペースを消す\n",
        "        return text\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq2ebKoOxThi"
      },
      "source": [
        "# リストに前処理した本文と、カテゴリーのラベルを追加していく\n",
        "import glob\n",
        "\n",
        "list_text = []\n",
        "list_label = []\n",
        "\n",
        "for cat in categories:\n",
        "    text_files = glob.glob(os.path.join(\"./data/livedoor/text\", cat, \"*.txt\"))\n",
        "\n",
        "    # 前処理extract_main_txtを実施して本文を取得\n",
        "    body = [extract_main_txt(text_file) for text_file in text_files]\n",
        "\n",
        "    label = [cat] * len(body)  # bodyの数文だけカテゴリー名のラベルのリストを作成\n",
        "\n",
        "    list_text.extend(body)  # appendが要素を追加するのに対して、extendはリストごと追加する\n",
        "    list_label.extend(label)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKPb_LJxxuOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7cc0b6-f757-4385-94ab-2581de261bb2"
      },
      "source": [
        "# 0番目の文章とラベルを確認\n",
        "print(list_text[0])\n",
        "print(list_label[0])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "スポーツメーカー大手デサント社による人気ゴルフブランド「ルコックゴルフ」では、ゴルファーをサポートする数々のウェアやアイテムをリリースしているのはご存知だろう。中でも、今年ゴルファーの間で一際大きな話題を集めているのが「ウルビルド4D」なる裁断技術を使用した最新ゴルフウェアになる。まず、ウルビルドとはドイツ語で原型という意味を持つ。3Dならぬ4Dは「4ディメンション」（4次元）のことを指し、3Dの立体裁断は静体位で静止した身体をベースに裁断してウェアを制作しているのに対し、4Dでは動きを想定した裁断法を取り入れているというわけだ。これまでなら、スイングなどの動作は、ウェアによって身体の動きが制限されてしまうこともあったが、この「ウルビルド4D」シリーズでは、これを極限まで抑え、可動域を大幅に広げているのが特徴で、2Dのものに比べれば、脇部分が上下運動で27％、肩・背中部分が水平方向運動で73％、内股部分は56％、臀部〜ひざ部分屈伸運動では46％も拡大したという。また、ボトムスもウルビルド4Dを採用しており、普通のパンツに比べると、その履き心地はもちろん、4Dで裁断したパーツを使用することで、しゃがんだりひざを曲げたときにウエストの部分が浮いたりするのをおさえ、身体にフィットした抜群の動きやすさを実現している。さらに、トップス＆ボトムスに共通して、通気性が非常に良いことも見逃せない。これらはソニックエアという、動くと風通りがよくなる素材を使用しており、汗を吸ってすばやく乾かすのと同時に衣服内の熱気を外に出すという気化熱を応用しているため、暑い日差しが照りつける夏場のプレーも快適＆清潔に——。それでいて、プレーの質が向上する「ウルビルド4D」シリーズ、この夏のプレーでアナタも試してみてはいかがだろうか。■関連リンク・ルコックゴルフ公式サイト・ゴルフ特集 2010 - livedoor HOMME■関連記事・ビームスがオススメするゴルフウェアの現在 【ゴルフ特集】・ゴルフ女子が選ぶ、人気キャディバッグ「OGIO」トップ3 【ゴルフ特集】・ボールにも色気を！ スーパーブルーがあなたを救う 【ゴルフ特集】・カモフラ柄でミスショットも“カモフラージュ” 【ゴルフ特集】\n",
            "livedoor-homme\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Kf1D9xxuvP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "4dbfcc85-e578-4ef2-ea98-c6f49e5d2a92"
      },
      "source": [
        "# pandasのDataFrameにする\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'text': list_text, 'label': list_label})\n",
        "\n",
        "# 大きさを確認しておく（7,376文章が存在）\n",
        "print(df.shape)\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7376, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>スポーツメーカー大手デサント社による人気ゴルフブランド「ルコックゴルフ」では、ゴルファーをサ...</td>\n",
              "      <td>livedoor-homme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ワークスペースのデザイニングに優れた企業のオフィスを訪問し、そこで働く人々の声から、その”働...</td>\n",
              "      <td>livedoor-homme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>「3年で転職は早すぎる？」「将来が見えない」「仕事が面白くない」・・・若手社会人の悩みは尽き...</td>\n",
              "      <td>livedoor-homme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>世界的DJであるFATBOY SLIM公認のビーチパーティー「BIG BEACH FESTI...</td>\n",
              "      <td>livedoor-homme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>フランスが世界に誇るブランド、エス・テー・デュポン社は、3月11日に発生した東日本大震災を受...</td>\n",
              "      <td>livedoor-homme</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text           label\n",
              "0  スポーツメーカー大手デサント社による人気ゴルフブランド「ルコックゴルフ」では、ゴルファーをサ...  livedoor-homme\n",
              "1  ワークスペースのデザイニングに優れた企業のオフィスを訪問し、そこで働く人々の声から、その”働...  livedoor-homme\n",
              "2  「3年で転職は早すぎる？」「将来が見えない」「仕事が面白くない」・・・若手社会人の悩みは尽き...  livedoor-homme\n",
              "3  世界的DJであるFATBOY SLIM公認のビーチパーティー「BIG BEACH FESTI...  livedoor-homme\n",
              "4  フランスが世界に誇るブランド、エス・テー・デュポン社は、3月11日に発生した東日本大震災を受...  livedoor-homme"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB8p83xi02ck",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "ac15f131-43d7-4223-b7fe-cb194f3185ba"
      },
      "source": [
        "# カテゴリーの辞書を作成\n",
        "dic_id2cat = dict(zip(list(range(len(categories))), categories))\n",
        "dic_cat2id = dict(zip(categories, list(range(len(categories)))))\n",
        "\n",
        "print(dic_id2cat)\n",
        "print(dic_cat2id)\n",
        "\n",
        "# DataFrameにカテゴリーindexの列を作成\n",
        "df[\"label_index\"] = df[\"label\"].map(dic_cat2id)\n",
        "df.head()\n",
        "\n",
        "# label列を消去し、text, indexの順番にする\n",
        "df = df.loc[:, [\"text\", \"label_index\"]]\n",
        "df.head()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'livedoor-homme', 1: 'it-life-hack', 2: 'topic-news', 3: 'movie-enter', 4: 'smax', 5: 'sports-watch', 6: 'dokujo-tsushin', 7: 'peachy', 8: 'kaden-channel'}\n",
            "{'livedoor-homme': 0, 'it-life-hack': 1, 'topic-news': 2, 'movie-enter': 3, 'smax': 4, 'sports-watch': 5, 'dokujo-tsushin': 6, 'peachy': 7, 'kaden-channel': 8}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>スポーツメーカー大手デサント社による人気ゴルフブランド「ルコックゴルフ」では、ゴルファーをサ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ワークスペースのデザイニングに優れた企業のオフィスを訪問し、そこで働く人々の声から、その”働...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>「3年で転職は早すぎる？」「将来が見えない」「仕事が面白くない」・・・若手社会人の悩みは尽き...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>世界的DJであるFATBOY SLIM公認のビーチパーティー「BIG BEACH FESTI...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>フランスが世界に誇るブランド、エス・テー・デュポン社は、3月11日に発生した東日本大震災を受...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label_index\n",
              "0  スポーツメーカー大手デサント社による人気ゴルフブランド「ルコックゴルフ」では、ゴルファーをサ...            0\n",
              "1  ワークスペースのデザイニングに優れた企業のオフィスを訪問し、そこで働く人々の声から、その”働...            0\n",
              "2  「3年で転職は早すぎる？」「将来が見えない」「仕事が面白くない」・・・若手社会人の悩みは尽き...            0\n",
              "3  世界的DJであるFATBOY SLIM公認のビーチパーティー「BIG BEACH FESTI...            0\n",
              "4  フランスが世界に誇るブランド、エス・テー・デュポン社は、3月11日に発生した東日本大震災を受...            0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaE_8vER18xY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2950cc73-b1af-43c4-acbc-dcc2e84eff77"
      },
      "source": [
        "# 順番をシャッフルする\n",
        "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>日本テレビ「ダウンタウンDX」（16日放送分）では、福岡ソフトバンクホークスリハビリ担当コー...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ここ1〜2年、店内の一角にシールコーナを設けている文具店や雑貨店が増えている。一歩踏み込むと...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1985年に公開された映画『バック・トゥ・ザ・フューチャー』でタイムマシンとして登場し、デロ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>先日、携帯電話を買い替えるために家電量販店へ行ったところ、担当の販売員の方が、柳原可奈子にそ...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>阪神タイガースの金本知憲外野手(43)の背番号6が、永久欠番に内定していると26日付のデイリ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label_index\n",
              "0  日本テレビ「ダウンタウンDX」（16日放送分）では、福岡ソフトバンクホークスリハビリ担当コー...            5\n",
              "1  ここ1〜2年、店内の一角にシールコーナを設けている文具店や雑貨店が増えている。一歩踏み込むと...            6\n",
              "2  1985年に公開された映画『バック・トゥ・ザ・フューチャー』でタイムマシンとして登場し、デロ...            3\n",
              "3  先日、携帯電話を買い替えるために家電量販店へ行ったところ、担当の販売員の方が、柳原可奈子にそ...            6\n",
              "4  阪神タイガースの金本知憲外野手(43)の背番号6が、永久欠番に内定していると26日付のデイリ...            5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRrt-XH72C3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92669127-a59d-4113-f369-198ca51fcf3a"
      },
      "source": [
        "# tsvファイルで保存する\n",
        "\n",
        "# 全体の2割の文章数\n",
        "len_0_2 = len(df) // 5\n",
        "\n",
        "# 前から2割をテストデータとする\n",
        "df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[:len_0_2].shape)\n",
        "\n",
        "# 前2割からを訓練&検証データとする\n",
        "df[len_0_2:].to_csv(\"./train_eval.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[len_0_2:].shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1475, 2)\n",
            "(5901, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0NykW7u3Mw9"
      },
      "source": [
        "# tsvファイルをダウンロードしたい場合\n",
        "from google.colab import files\n",
        "\n",
        "# ダウンロードする場合はコメントを外す\n",
        "# 少し時間がかかる（4MB）\n",
        "# files.download(\"./test.tsv\")\n",
        "\n",
        "\n",
        "# ダウンロードする場合はコメントを外す\n",
        "# 少し時間がかかる（18MB）\n",
        "# files.download(\"./train_eval.tsv\")\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPT3Pjr94oPW"
      },
      "source": [
        "## 準備2：LivedoorニュースをBERT用のDataLoaderにする\n",
        "\n",
        "Hugginfaceのリポジトリの案内とは異なり、torchtextを使用した手法で実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXX4pr2-kY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487b9cbf-9bc9-4c35-8889-cc4e829f921b"
      },
      "source": [
        "# MeCabとtransformersの用意\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "!pip install transformers==2.9.0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "aptitude is already the newest version (0.8.10-6ubuntu1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "No packages will be installed, upgraded, or removed.\n",
            "0 packages upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 0 B of archives. After unpacking 0 B will be used.\n",
            "                            \n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: transformers==2.9.0 in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (0.1.96)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R2N702w0mVy"
      },
      "source": [
        "!sudo cp /etc/mecabrc /usr/local/etc/"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tKqo2TF9vzj"
      },
      "source": [
        "import torch\n",
        "import torchtext  # torchtextを使用\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "\n",
        "# 日本語BERTの分かち書き用tokenizerです\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
        "    'bert-base-japanese-whole-word-masking')\n",
        "# # 日本語BERTの分かち書き用tokenizerです\n",
        "# tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZypBWaE-PB6"
      },
      "source": [
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
        "\n",
        "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
        "\n",
        "\n",
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
        "\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,\n",
        "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# (注釈)：各引数を再確認\n",
        "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
        "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
        "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
        "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
        "# include_length: 文章の単語数のデータを保持するか\n",
        "# batch_first：ミニバッチの次元を用意するかどうか\n",
        "# fix_length：全部の文章をfix_lengthと同じ長さになるように、paddingします\n",
        "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyLNL-sd_Xd5"
      },
      "source": [
        "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
        "# 少し時間がかかります\n",
        "# train_eval：5901個、test：1475個\n",
        "dataset_train_eval, dataset_test = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='.', train='train_eval.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNRofv_iDOYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e04088b-5941-4a55-ef94-d584675c2942"
      },
      "source": [
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "# train_eval：5901個、test：1475個\n",
        "\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(\n",
        "    split_ratio=1.0 - 1475/5901, random_state=random.seed(1234))\n",
        "\n",
        "# datasetの長さを確認してみる\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4426\n",
            "1475\n",
            "1475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_XilciAI2la",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9071b73d-1c6a-4baf-fcd1-e10505cfa232"
      },
      "source": [
        "# datasetの中身を確認してみる\n",
        "item = next(iter(dataset_train))\n",
        "print(item.Text)\n",
        "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
        "print(\"ラベル：\", item.Label)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([    2,   599,     5,   640,     9,     6,  1568,  2032, 15872,    18,\n",
            "         1628, 28452, 28512,     8, 12614,    11,  1330,    16,  2786,    84,\n",
            "         3318,     6,  7052,     7, 13770,     6,   599,   271,    64,     6,\n",
            "         8583,    15,    10,   640,    11,  4166,  2156,  4627,     8,   687,\n",
            "        28536,    36,   599,  9551,     5,     9, 14596,    12,     6,  9749,\n",
            "        21910, 28447, 14406, 28504,    58,  3051, 28489,    38,   140,    53,\n",
            "            9,   707,     5,    12,     9, 17230,    55, 23749,    13,    15,\n",
            "           10, 15221,    12, 19235,  3877,    18,   599,    11,  7138,    82,\n",
            "            7,     9,     6,  1040,     5, 13253,    11,  7881,    34,    45,\n",
            "           14, 11047,     8,  2909,  7168,     9,     6,  6724,  5464,    12,\n",
            "        22465,     7,  7852,   392,     6, 11255, 11214,    35,  3265, 30161,\n",
            "         4437,    11,  1951,    15,  2610,     8, 16062, 11255, 11214,  4437,\n",
            "           12,  2333, 29630,   742, 28700,   679, 28496,  2340,  2383,     5,\n",
            "        11255, 11214,    12,     9,  2786,    84,    80,  3286,   140,    53,\n",
            "            7,  3148, 28466, 28542,    18,     5,    14,     6,  6724,  5464,\n",
            "            5,  1197,    11, 10318,    10, 16062, 11255, 11214,  4437,     8,\n",
            "        10892,    49,  1044,  8036,  9736,  4194,    13,     5, 10072,  4437,\n",
            "           49,     6, 11908,    11,  2110,    10, 13253, 10320,  1251,  4437,\n",
            "           12,     6,   599,     5, 15221,    11,    55, 23749,    26,   191,\n",
            "           16,  4481, 17015,   205,     8, 19305, 19696, 28174,    23,   725,\n",
            "           24,  3152,  5975, 13268,   228, 12526,   143, 10892,     5, 15172,\n",
            "           13, 10072,    15,    10, 11255, 11214,  4437,     8,  2416,   153,\n",
            "           11,  1374,    15,    10,   640,     7, 26591,  2786,    80,    13,\n",
            "            6, 12650,  2922,   485, 20652, 25045,    14, 11299,     7,  6432,\n",
            "           26,    20,    16,  4282,  2610,     8, 20652, 25045,     9,     6,\n",
            "           36,  2128,     5,  2679,   384,    38,    13,    36,  9962,  4187,\n",
            "          384,    38,    64,     5,  5061,    40, 11830,     7,  2430,    26,\n",
            "           20,     6,    59,   276, 25482,   482,  3371,  1897,   679,  1972,\n",
            "            6,    36,   598, 28450,    26,     6,  1259, 28544, 29333, 28544,\n",
            "           15,    10,  2794,   816,    16,     6,  7553,    11,  2501, 13592,\n",
            "        21834,   900, 29360,   249, 28522,  4551,  1058, 11655,  2935,  5880,\n",
            "        26984,    38,    64,     6,  3005,  8741,   547, 29410,     5, 20652,\n",
            "        25045,    14,  6432,    26,    62,    45,    28, 15743, 10150, 20652,\n",
            "        25045,    14,  6432,    26,    62,    29, 11183,    80,  7952, 28467,\n",
            "            5, 13447,    12,     6, 13734,  2786,   342,    45, 11013,  3350,\n",
            "         2992,     8, 19696, 28174,    23, 18060,    24,  4691,   465,  3152,\n",
            "         5975, 13268,   228, 12526,   143, 31176,  5870, 20211,  3152,  5946,\n",
            "        14928,  4974,    23,   725,    24,   871,  1044,    57,   936,  1219,\n",
            "        14144, 28535, 12526,   143, 11047,    18,  6053,    14,    31,    32,\n",
            "            6,    36,   205,  9409,  4492, 30210,    15, 26985,    10,  3286,\n",
            "        15060,  1903,     9,   130,  6769,  1058,    29,  2935,  4799,    53,\n",
            "            7,  3148, 28466, 28542,    18,     5,    14,     6,    17,   181,\n",
            "            5,  2416,   153,    11, 12455,    13,  5585,    34, 11255, 11214,\n",
            "         4437,  2992,     8,  1044,  8036,  9736,  4194,     6, 10892,    64,\n",
            "           11,   406,    15,    16,     6,  1091,    12,  2416,   153,    11,\n",
            "         5585,     8,  1091,   186,    12,  2786,    16,    21,    80,    53,\n",
            "           11,  1961,    15,   790,     6,    36, 11707,  6316,    38,    11,\n",
            "         4130, 11384,   790,    34,    45,    14,   203,  2610,     8, 21870,\n",
            "           49,  3807,  5360,    32,    64,     6,  1403,    18,    73, 28540,\n",
            "          663,     5,    32,     9,     6,   599,    40,  7309,    12,  2153,\n",
            "           15,    16,   546,    16,   679,  5870, 20211,  3152,  5946, 14928,\n",
            "         4974,     3])\n",
            "長さ： 512\n",
            "ラベル： 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7323f5aJFzz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5e6c4299-feae-488b-f8e1-9a2f94128a3a"
      },
      "source": [
        "# datasetの中身を文章に戻し、確認\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(item.Text.tolist()))  # 文章\n",
        "dic_id2cat[int(item.Label)]  # id\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '朝', 'の', '時間', 'は', '、', '女子', 'にとって', '大事', 'な', 'ひと', '##と', '##き', '。', '余裕', 'を', '持っ', 'て', '起き', 'られ', 'たら', '、', '食事', 'に', 'メイク', '、', '朝', '活', 'など', '、', '充実', 'し', 'た', '時間', 'を', '過ご', '##せる', 'はず', '。', 'だけ', '##ど', '「', '朝', '起きる', 'の', 'は', '苦手', 'で', '、', 'いつも', '慌', '##た', '##だし', '##く', 'なっ', 'ちゃ', '##う', '」', 'という', '人', 'は', '多い', 'の', 'で', 'は', '!?', 'ス', '##ッキリ', 'と', 'し', 'た', '目覚め', 'で', 'さわ', '##やか', 'な', '朝', 'を', '迎える', 'ため', 'に', 'は', '、', '自分', 'の', '睡眠', 'を', 'コントロール', 'する', 'こと', 'が', '大切', '。', 'そこで', '今回', 'は', '、', 'スマート', 'フォン', 'で', '手軽', 'に', 'ダウンロード', 'できる', '、', '目覚', '##まし', '・', '快', '##眠', 'アプリ', 'を', '紹介', 'し', 'ます', '。', 'ユニーク', '目覚', '##まし', 'アプリ', 'で', '遅', '##刻', '知ら', '##ず', '!', '##「', 'もう', '普通', 'の', '目覚', '##まし', 'で', 'は', '起き', 'られ', 'ない', '!」', 'という', '人', 'に', 'オス', '##ス', '##メ', 'な', 'の', 'が', '、', 'スマート', 'フォン', 'の', '機能', 'を', '活かし', 'た', 'ユニーク', '目覚', '##まし', 'アプリ', '。', 'Twitter', 'や', 'f', '##ace', '##bo', '##ok', 'と', 'の', '連動', 'アプリ', 'や', '、', 'センサー', 'を', '使っ', 'た', '睡眠', 'サイクル', '管理', 'アプリ', 'で', '、', '朝', 'の', '目覚め', 'を', 'ス', '##ッキリ', 'さ', 'せ', 'て', 'いき', 'ましょ', 'う', '。', '●', 'OK', '##ITE', '(', 'c', ')', 'e', '##ure', '##ka', ',', 'Inc', '.', 'Twitter', 'の', 'アカウント', 'と', '連動', 'し', 'た', '目覚', '##まし', 'アプリ', '。', 'アラ', '##ーム', 'を', '設定', 'し', 'た', '時間', 'に', 'ちゃんと', '起き', 'ない', 'と', '、', '恥', '##ずか', '##しい', 'つぶ', '##やき', 'が', '勝手', 'に', '投稿', 'さ', 'れ', 'て', 'しまい', 'ます', '。', 'つぶ', '##やき', 'は', '、', '「', '裏', 'の', '顔', '系', '」', 'と', '「', '不思議', 'ちゃん', '系', '」', 'など', 'の', 'ジャンル', 'から', 'ランダム', 'に', '選択', 'さ', 'れ', '、', 'その', '数', 'なんと', '約', '1000', '種類', '!', '例えば', '、', '「', '要', '##は', 'さ', '、', 'ふ', '##わ', '##ふ', '##わ', 'し', 'た', '服', '着', 'て', '、', '髪', 'を', 'くる', '##くる', '巻い', 'とき', '##ゃ', 'モ', '##テ', 'る', 'ん', 'でしょ', '?', '簡単', 'じゃん', '」', 'など', '、', 'イメージ', '##ダウン', '必', '##至', 'の', 'つぶ', '##やき', 'が', '投稿', 'さ', 'れる', 'こと', 'も', '...。', 'どんな', 'つぶ', '##やき', 'が', '投稿', 'さ', 'れる', 'か', 'わから', 'ない', 'スリ', '##ル', 'の', 'おかげ', 'で', '、', 'しっかり', '起き', 'られる', 'こと', '間違い', 'なし', 'です', '。', 'OK', '##ITE', '(', 'iPhone', ')', '無料', '/', 'e', '##ure', '##ka', ',', 'Inc', '.', '##●', 'al', '##arm', 'e', '##ver', '##yo', '##ne', '(', 'c', ')', '2011', 'f', '4', 's', '##am', '##ura', '##i', 'Inc', '.', '大切', 'な', '約束', 'が', 'ある', '日', '、', '「', 'う', '##っかり', '寝', '##坊', 'し', 'ちゃっ', 'た', '!」', 'なんて', '経験', 'は', 'あり', 'ませ', 'ん', 'か', '?', 'そんな', '人', 'に', 'オス', '##ス', '##メ', 'な', 'の', 'が', '、', '1', 'つ', 'の', 'アラ', '##ーム', 'を', '友達', 'と', '共有', 'する', '目覚', '##まし', 'アプリ', 'です', '。', 'f', '##ace', '##bo', '##ok', '、', 'Twitter', 'など', 'を', '使用', 'し', 'て', '、', 'グループ', 'で', 'アラ', '##ーム', 'を', '共有', '。', 'グループ', '内', 'で', '起き', 'て', 'い', 'ない', '人', 'を', '確認', 'し', 'たり', '、', '「', 'モーニング', 'メッセージ', '」', 'を', '送り', '合っ', 'たり', 'する', 'こと', 'が', 'でき', 'ます', '。', 'デート', 'や', '旅行', '出発', '日', 'など', '、', '特別', 'な', 'お', '##出', '##かけ', 'の', '日', 'は', '、', '朝', 'から', 'みんな', 'で', '協力', 'し', 'て', 'み', 'て', '!', 'al', '##arm', 'e', '##ver', '##yo', '##ne', '[SEP]']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'peachy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qiBmmdsJ-aK"
      },
      "source": [
        "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
        "batch_size = 16  # BERTでは16、32あたりを使用する\n",
        "\n",
        "dl_train = torchtext.legacy.data.Iterator(\n",
        "    dataset_train, batch_size=batch_size, train=True)\n",
        "\n",
        "dl_eval = torchtext.legacy.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.legacy.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-sNpiK5K14s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd3dd1f-687d-45f4-b986-52e8dd989fc9"
      },
      "source": [
        "# DataLoaderの動作確認 \n",
        "\n",
        "batch = next(iter(dl_test))\n",
        "print(batch)\n",
        "print(batch.Text[0].shape)\n",
        "print(batch.Label.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 16]\n",
            "\t[.Text]:('[torch.LongTensor of size 16x512]', '[torch.LongTensor of size 16]')\n",
            "\t[.Label]:[torch.LongTensor of size 16]\n",
            "torch.Size([16, 512])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8e6NhQ3cLcq"
      },
      "source": [
        "## 準備3：BERTのクラス分類用のモデルを用意する\n",
        "\n",
        "Huggingfaceさんのをそのまま使うのではなく、BERTのbaseだけ使い、残りは自分で実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFlvnI05a4xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea16ff5-935f-4373-faf5-0b9b24f07e7a"
      },
      "source": [
        "from transformers.modeling_bert import BertModel\n",
        "\n",
        "# BERTの日本語学習済みパラメータのモデルです\n",
        "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')\n",
        "print(model)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BgGd7fLPssV"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class BertForLivedoor(nn.Module):\n",
        "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForLivedoor, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は9クラス\n",
        "        self.cls = nn.Linear(in_features=768, out_features=9)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
        "        output = self.cls(vec_0)  # 全結合層\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOtveynWRwK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d253f844-27f3-4efb-8353-b8230bc604f7"
      },
      "source": [
        "# モデル構築\n",
        "net = BertForLivedoor()\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ネットワーク設定完了\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkg7r2RIR7qU"
      },
      "source": [
        "## 準備4：BERTのファインチューニングの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPGYvX4RR3UT"
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in net.cls.parameters():\n",
        "    param.requires_grad = True\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il_-wow4Suwe"
      },
      "source": [
        "# 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
        "])\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu7KRn1bTIQp"
      },
      "source": [
        "## 5. 訓練を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNaAXgiITFiw"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs)\n",
        "\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfnH-gAmS75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bd64be-4a23-4ee0-bd7c-f60044166d51"
      },
      "source": [
        "# 学習・検証を実行する。1epochに2分ほどかかります\n",
        "num_epochs = 4\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 2.1614 || 10iter. || 本イテレーションの正解率：0.125\n",
            "イテレーション 20 || Loss: 1.9659 || 10iter. || 本イテレーションの正解率：0.3125\n",
            "イテレーション 30 || Loss: 1.8514 || 10iter. || 本イテレーションの正解率：0.5\n",
            "イテレーション 40 || Loss: 1.1241 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 50 || Loss: 1.2962 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 60 || Loss: 0.8704 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 70 || Loss: 0.9895 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 80 || Loss: 0.7007 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 90 || Loss: 0.5129 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 100 || Loss: 0.8516 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 110 || Loss: 0.7194 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 120 || Loss: 0.6843 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 130 || Loss: 0.9372 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 140 || Loss: 0.4547 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 150 || Loss: 0.3959 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 160 || Loss: 0.2990 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 170 || Loss: 0.7105 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 180 || Loss: 0.3923 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 190 || Loss: 0.3818 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 200 || Loss: 0.4815 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 210 || Loss: 0.2688 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 220 || Loss: 0.4599 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 230 || Loss: 0.3241 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 240 || Loss: 0.5669 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 250 || Loss: 0.4813 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 260 || Loss: 0.1050 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 270 || Loss: 0.5321 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "Epoch 1/4 | train |  Loss: 0.8172 Acc: 0.7402\n",
            "Epoch 1/4 |  val  |  Loss: 0.3751 Acc: 0.8678\n",
            "イテレーション 10 || Loss: 0.3918 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 20 || Loss: 0.2314 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 30 || Loss: 0.3295 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 40 || Loss: 0.5171 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 50 || Loss: 0.3348 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 60 || Loss: 0.1046 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 70 || Loss: 1.1061 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 80 || Loss: 0.5884 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 90 || Loss: 0.4257 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 100 || Loss: 0.2539 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 110 || Loss: 0.2354 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 120 || Loss: 0.4670 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 130 || Loss: 0.5042 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 140 || Loss: 0.2529 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 150 || Loss: 0.3559 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 160 || Loss: 0.1950 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 170 || Loss: 0.3048 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 180 || Loss: 0.1625 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 190 || Loss: 0.1578 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 200 || Loss: 0.5563 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 210 || Loss: 0.4222 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 220 || Loss: 0.6041 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 230 || Loss: 0.4959 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 240 || Loss: 0.2437 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 250 || Loss: 0.1869 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 260 || Loss: 0.0504 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 270 || Loss: 0.4248 || 10iter. || 本イテレーションの正解率：0.875\n",
            "Epoch 2/4 | train |  Loss: 0.3232 Acc: 0.8992\n",
            "Epoch 2/4 |  val  |  Loss: 0.2950 Acc: 0.9031\n",
            "イテレーション 10 || Loss: 0.0683 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 20 || Loss: 0.1432 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 30 || Loss: 0.0273 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 40 || Loss: 0.3030 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 50 || Loss: 0.0781 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 60 || Loss: 0.2876 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 70 || Loss: 0.3237 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 80 || Loss: 0.4372 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 90 || Loss: 0.1851 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 100 || Loss: 0.0333 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 110 || Loss: 0.4085 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 120 || Loss: 0.1116 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 130 || Loss: 0.2182 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 140 || Loss: 0.1291 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 150 || Loss: 0.6870 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 160 || Loss: 0.3028 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 170 || Loss: 0.3606 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 180 || Loss: 0.2651 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 190 || Loss: 0.1265 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 200 || Loss: 0.3210 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 210 || Loss: 0.0261 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 220 || Loss: 0.1367 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 230 || Loss: 0.2479 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 240 || Loss: 0.4174 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 250 || Loss: 0.5064 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 260 || Loss: 0.1225 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 270 || Loss: 0.0474 || 10iter. || 本イテレーションの正解率：1.0\n",
            "Epoch 3/4 | train |  Loss: 0.2220 Acc: 0.9295\n",
            "Epoch 3/4 |  val  |  Loss: 0.2654 Acc: 0.9132\n",
            "イテレーション 10 || Loss: 0.0763 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 20 || Loss: 0.0243 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 30 || Loss: 0.0610 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 40 || Loss: 0.0977 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 50 || Loss: 0.0503 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 60 || Loss: 0.1476 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 70 || Loss: 0.0337 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 80 || Loss: 0.0723 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 90 || Loss: 0.2134 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 100 || Loss: 0.0083 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 110 || Loss: 0.0796 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 120 || Loss: 0.0530 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 130 || Loss: 0.1620 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 140 || Loss: 0.0902 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 150 || Loss: 0.0376 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 160 || Loss: 0.0507 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 170 || Loss: 0.4159 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 180 || Loss: 0.1152 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 190 || Loss: 0.1533 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 200 || Loss: 0.4088 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 210 || Loss: 0.0573 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 220 || Loss: 0.0814 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 230 || Loss: 0.0151 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 240 || Loss: 0.2956 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 250 || Loss: 0.0497 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 260 || Loss: 0.2654 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 270 || Loss: 0.2543 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "Epoch 4/4 | train |  Loss: 0.1610 Acc: 0.9462\n",
            "Epoch 4/4 |  val  |  Loss: 0.3302 Acc: 0.9003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zuZC_0yaOs"
      },
      "source": [
        "## テストデータでの性能を確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdxKZzijT5PG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adaf256b-f3a2-407f-cf11-5a303a56f3b7"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "\n",
        "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # BertForLivedoorに入力\n",
        "        outputs = net_trained(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)  # 損失を計算\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 93/93 [00:55<00:00,  1.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "テストデータ1475個での正解率：0.9024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}